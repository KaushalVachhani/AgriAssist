{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b92ca350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import io\n",
    "from PIL import Image\n",
    "from gtts import gTTS\n",
    "import tempfile\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from groq import Groq\n",
    "import elevenlabs\n",
    "from elevenlabs.client import ElevenLabs\n",
    "import platform\n",
    "import subprocess\n",
    "from uuid import uuid4\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1019aa48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you today?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()  # Load environment variables from a .env file if present\n",
    "\n",
    "if os.getenv(\"GROQ_API_KEY\") is None:\n",
    "    raise ValueError(\"Please set the GROQ_API_KEY environment variable.\")\n",
    "if os.getenv(\"GOOGLE_API_KEY\") is None:\n",
    "    raise ValueError(\"Please set the GOOGLE_API_KEY environment variable.\")\n",
    "if os.getenv(\"ELEVENLABS_API_KEY\") is None:\n",
    "    raise ValueError(\"Please set the ELEVENLABS_API_KEY environment variable.\")\n",
    "\n",
    "model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "model.generate_content(\"Hello\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f407f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_multimodal_query(text_input, image_input, audio_input=None):\n",
    "    \"\"\"\n",
    "    Handles farming-related queries by processing a combination of text,\n",
    "    image, and audio input using a Generative AI model.\n",
    "\n",
    "    Args:\n",
    "        text_input (str): The text description of the problem.\n",
    "        image_input (PIL.Image.Image): A PIL Image object of the crop or pest.\n",
    "        audio_input (str): The file path to a recorded audio file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the AI's text response and the path to\n",
    "               the generated audio file.\n",
    "    \"\"\"\n",
    "    print(\"Received inputs:\")\n",
    "    print(f\"Text input: {text_input}\")\n",
    "    if image_input:\n",
    "        print(f\"Image input: {image_input}\")\n",
    "    if audio_input:\n",
    "        print(f\"Audio input: {audio_input}\")\n",
    "    question_id = str(uuid4())\n",
    "    try:\n",
    "        # --- Handle Audio Input (Placeholder for Transcription) ---\n",
    "        # NOTE: A real-world application would use a speech-to-text model here\n",
    "        # to transcribe the audio into text. For this demo, we'll simply\n",
    "        # acknowledge the audio file and process the other inputs.\n",
    "        if audio_input:\n",
    "            print(f\"Audio file received at: {audio_input}\")\n",
    "            # For a real implementation, you would do something like:\n",
    "            stt_model=\"whisper-large-v3\"\n",
    "            transcribed_text = transcribe_with_groq(stt_model, audio_input, os.getenv(\"GROQ_API_KEY\"))\n",
    "            text_input = f\"{text_input}\\nTranscribed audio: {transcribed_text}\"\n",
    "\n",
    "        # --- Construct the Multimodal Prompt ---\n",
    "        prompt_parts = [\n",
    "            \"You are an AI assistant specialized in providing advice to farmers. \"\n",
    "            \"Analyze the following information to answer the farmer's question. \"\n",
    "            \"Be practical and concise. If you cannot provide a specific answer, \"\n",
    "            \"provide general helpful advice. Keep your answer concise (max 2 sentences). No preamble, start your answer right away please. No astricks please.\",\n",
    "            f\"\\n\\nFarmer's Question: {text_input}\"\n",
    "        ]\n",
    "\n",
    "        if image_input:\n",
    "            # If an image is provided, add it to the prompt parts for the model\n",
    "            # to analyze alongside the text.\n",
    "            prompt_parts.append(image_input)\n",
    "\n",
    "        # --- Call the Generative AI Model ---\n",
    "        print(\"Sending request to Gemini model...\")\n",
    "        response = model.generate_content(prompt_parts)\n",
    "        ai_text_response = response.text\n",
    "        print(f\"AI Response generated: {ai_text_response}\")\n",
    "\n",
    "        # --- Convert AI Text Response to Speech ---\n",
    "        print(\"Converting AI response to speech...\")\n",
    "        output_filepath = f\"generated_speech_responses/speech_response_{question_id}.mp3\"\n",
    "        text_to_speech_with_elevenlabs(input_text=ai_text_response, output_filepath=output_filepath)\n",
    "        print(f\"Generated speech saved to: {output_filepath}\")\n",
    "        return ai_text_response, output_filepath\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"An error occurred: {e}\"\n",
    "        print(error_message)\n",
    "        return error_message, None\n",
    "    \n",
    "def transcribe_with_groq(stt_model, audio_filepath, GROQ_API_KEY):\n",
    "    client=Groq(api_key=GROQ_API_KEY)\n",
    "    \n",
    "    audio_file=open(audio_filepath, \"rb\")\n",
    "    transcription=client.audio.transcriptions.create(\n",
    "        model=stt_model,\n",
    "        file=audio_file,\n",
    "        language=\"en\"\n",
    "    )\n",
    "    print(transcription)\n",
    "    return transcription.text\n",
    "\n",
    "def text_to_speech_with_elevenlabs(input_text, output_filepath):\n",
    "    client=ElevenLabs(api_key=os.getenv(\"ELEVENLABS_API_KEY\"))\n",
    "    audio=client.text_to_speech.convert(\n",
    "        text= input_text,\n",
    "        voice_id=\"aGb0TwKthRLQTPThYRqI\",\n",
    "        output_format=\"mp3_44100_128\",\n",
    "        model_id=\"eleven_turbo_v2\"\n",
    "    )\n",
    "    elevenlabs.save(audio, output_filepath)\n",
    "    os_name = platform.system()\n",
    "    try:\n",
    "        if os_name == \"Darwin\":  # macOS\n",
    "            subprocess.run(['afplay', output_filepath])\n",
    "        elif os_name == \"Windows\":  # Windows\n",
    "            subprocess.run(['powershell', '-c', f'(New-Object Media.SoundPlayer \"{output_filepath}\").PlaySync();'])\n",
    "        elif os_name == \"Linux\":  # Linux\n",
    "            subprocess.run(['aplay', output_filepath])  # Alternative: use 'mpg123' or 'ffplay'\n",
    "        else:\n",
    "            raise OSError(\"Unsupported operating system\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while trying to play the audio: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bda5565b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7872\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Step 4: Build the Gradio Interface ---\n",
    "# Define the input components for the UI.\n",
    "text_box = gr.Textbox(\n",
    "    label=\"Describe your farming issue:\", \n",
    "    placeholder=\"e.g., 'My tomato plant leaves are turning yellow.'\", \n",
    "    interactive=True\n",
    ")\n",
    "image_box = gr.Image(\n",
    "    type=\"pil\", \n",
    "    label=\"Upload an image of the plant (optional):\", \n",
    "    interactive=True\n",
    ")\n",
    "audio_box = gr.Audio(\n",
    "    label=\"Record your question (optional):\", \n",
    "    sources=[\"microphone\"], \n",
    "    interactive=True,\n",
    "    type=\"filepath\"\n",
    ")\n",
    "\n",
    "# Define the output components for the UI.\n",
    "text_output = gr.Textbox(\n",
    "    label=\"AI Response (Text):\", \n",
    "    placeholder=\"The AI's response will appear here...\", \n",
    "    interactive=False\n",
    ")\n",
    "audio_output = gr.Audio(\n",
    "    label=\"AI Response (Audio):\", \n",
    "    interactive=False,\n",
    "    type=\"filepath\"   # Explicitly state you'll return a filepath\n",
    ")\n",
    "\n",
    "# Create the Gradio interface.\n",
    "gr.Interface(\n",
    "    fn=handle_multimodal_query,\n",
    "    inputs=[text_box, image_box, audio_box],\n",
    "    outputs=[text_output, audio_output],\n",
    "    title=\"Farm AI Assistant Demo\",\n",
    "    description=\"Ask a farming question and get a text and audio response. \"\n",
    "                \"You can use text, an image, or both.\",\n",
    ").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41605df6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agriassist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
